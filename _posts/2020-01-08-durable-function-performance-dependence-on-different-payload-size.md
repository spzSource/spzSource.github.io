---
layout: post
title: "Azure Durable Functions: performance tips"
date: 2020-01-09 23:30:00 +0300
categories: azure
tags: functions, azure, durable framework, performance, throughput
---

I've been using Azure Durable Functions extension for a long time for developing a hihgh load ETL process. And I have to mention that it is a very useful library that allows you write stateful functions in a serverless compute environment. Also it not worse to mention that programming model allows you to write code in a same way you write asynchronous code using `async` and `await` keywords. At a first look it might seem that there is no difference between regular async calls and durable calls, but it's not true.

Regular asynchronous code is handled by compiler by generating async state machine, so that async method is able to resume at the place where execution was suspended previously. But in case Durable Function (orchestrator) we do not have state machine generated by compiler. Instead the framework preserve execution history in external storage (Azure Storage) and then every time when orchestration function "wakes up" it "plays back" execution history in order to determine what operations have already been executed and what operations have to be executed.

And sometimes misunderstanding of how durable function works behind the scene leads to a huge performance issues. Below I've gathered some basic tips which migh help you to get rid of such performance issues.

## Tip #1: Payload size for Activity function does matter

Lets take a look at code sample below:

```cs
[FunctionName("Orchestrator")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    var items = await context.CallActivityAsync<string>("GetItems");

    foreach (var item in items)
    {
        await context.CallActivityAsync<string>("ProcessItem", item);
    }
}
```

And using this sample lets step back and bethink of how durable function framework works under the hood.

Lets say that activity function `GetItems` returns 500 MiB array of string. Since durable function framework stores all events happened in scope of orchestration function, this huge payload must be stored in Durable Storage. Then this 500 MiB of string are processed by `ProcessItem` activities one by one.

In a worst case scenario every time when `await context.CallActivityAsync<string>("ProcessItem", item);` is called, the orchestration function is unloaded from memory until `ProcessItem` function completed. When `ProcessItem` finished, the orchestration function wakes up and re-plays execution history in order to get actual execution state. In other words orchestration function starts from very start and tries to call `GetItems` and `ProcessItem` once again.

But it doesn't mean that `GetItems` function will be executed one more time. Instead of that the orchestration function fetches result of `GetItems` from Durable Storage. (The key point here is that `await context.CallActivityAsync<string>("GetItems")` might be executed multiple time, but activity function `GetItems` is executed only once).

I guess the issue here is obvious. Basically each time when orchestration function wakes up it must fetch 500 MiB payload from Durable storage. And this will happen as many times as how many elements in `items` array. As a result we just "burn" significant amount of time for downloading payload from Azure Storage rather that doing useful processing.

To be more practical lets run two tests:

#### Test #1 - Huge payload

```cs
    [FunctionName("SeqWithHugePayload")]
    public static async Task RunSeqHuge(
        [OrchestrationTrigger] IDurableOrchestrationContext context)
    {
        var payload = await context.CallActivityAsync<string[]>("GetHugePayload", null);

        await context.Measure(async () =>
        {
            foreach (var item in payload.Take(3))
            {
                await context.CallActivityAsync("ProcessItem", item);
            }
        });
    }

    [FunctionName("GetHugePayload")]
    public static Task<string[]> RunGetHugePayload([ActivityTrigger] IDurableActivityContext context, ILogger client)
    {
        var results = new List<string>();

        for (var i = 0; i < 100000; i++)
        {
            results.Add(Enumerable
                .Repeat(Guid.NewGuid().ToString(), 100)
                .Aggregate(new StringBuilder(), (builder, s) => builder.Append(s))
                .ToString());
        }

        return Task.FromResult(results.ToArray());
    }

```

In sample above you might see that function `GetHugePayload` returns array of 100000 elements, each of them is a string presented as a concatenation of 100 guids. Payload size is 144120004 bytes (~145 Mb).
Execution time to `SeqWithHugePayload` is 00:00:41:5714 (41 seconds).

#### Test #2 - Small payload

```cs
    [FunctionName("SeqWithSmallPayload")]
    public static async Task RunSeqHuge(
        [OrchestrationTrigger] IDurableOrchestrationContext context)
    {
        var payload = await context.CallActivityAsync<string[]>("GetSmallPayload", null);

        await context.Measure(async () =>
        {
            foreach (var item in payload.Take(3))
            {
                await context.CallActivityAsync("ProcessItem", item);
            }
        });
    }

    [FunctionName("GetSmallPayload")]
    public static Task<string[]> RunGetHugePayload([ActivityTrigger] IDurableActivityContext context, ILogger client)
    {
        var results = new List<string>();

        for (var i = 0; i < 10; i++)
        {
            results.Add(Enumerable
                .Repeat(Guid.NewGuid().ToString(), 100)
                .Aggregate(new StringBuilder(), (builder, s) => builder.Append(s))
                .ToString());
        }

        return Task.FromResult(results.ToArray());
    }

```

In sample above you might see that function `GetSmallPayload` returns array of 10 elements, each of them is a string presented as a concatenation of 100 guids. Payload size is 144124 bytes (~0.2 Mb).
Execution time to `SeqWithSmallPayload` is 00:00:03:3554 (4 seconds).

Table below represents results of both tests:

| Test  | Execution Time  |
|---|---|
| Huge Payload (145 Mb)  | 00:00:41:5714 (41 seconds)  |
| Small Payload (0.2 Mb)  |  00:00:03:3554 (4 seconds) |

#### How to optimize?

There are multiple options:
- use [Fan-out/fan-in](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp#fan-in-out) pattern, so that it drastically reduces number of orchestration functions replays. The less replays we have - the less round-trips to Azure storage we have to do, the better execution time;
- change design of the application to reduce the number of large messages (payloads);
- use [extended sessions](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-perf-and-scale#extended-sessions), which will potentially reduce the number of replays and therefore the number of times we need to fetch payload from Azure Storage.